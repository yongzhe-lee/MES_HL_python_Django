from asyncio.windows_events import NULL
import math
from winreg import KEY_WOW64_32KEY
import numpy as np
import pandas as pd
import json
import plotly

#import matplotlib.pyplot as plt
#import seaborn as sns
#from matplotlib.figure import Figure

from sklearn.linear_model import LinearRegression

from configurations import settings
from domain.models.da import DsModel, DsModelColumn, DsTagCorrelation
from domain.models.system import AttachFile
from domain.models.da import DsModelData
from domain.services.logging import LogWriter
from domain.services.common import CommonUtil
from domain.services.sql import DbUtil
from domain.services.file.attach_file import AttachFileService
from domain.services.calculation.data_analysis import DaService

# 24.07.23 ê¹€í•˜ëŠ˜ ì¶”ê°€ ì•Œê³ ë¦¬ì¦˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier
from sklearn.metrics import accuracy_score

# 24.07.23 ê¹€í•˜ëŠ˜ ì¶”ê°€ ì‹œê°í™”ë¥¼ ìœ„í•œ import
import matplotlib.pyplot as plt
from matplotlib import cm
from matplotlib.colors import to_hex
import plotly.graph_objs as go
import plotly.io as pio
import io
import urllib, base64

# 24.07.31 ê¹€í•˜ëŠ˜ ì¶”ê°€ ì²¨ë¶€íŒŒì¼ ë£¨íŠ¸ ì£¼ì†Œë¥¼ ê°€ì ¸ì˜¤ê¸°
# from configurations import settings


# 24.08.08 ê¹€í•˜ëŠ˜ ì¶”ê°€ PCAì²˜ë¦¬ ë©”ì„œë“œ ë”°ë¡œ ì¶”ì¶œ
def perform_pca(data, features, target_column, num_components=None, scale_factor=None):
    """
    PCA ìˆ˜í–‰ ë° ì£¼ì„±ë¶„ ì‹œê°í™” ë°ì´í„°ë¥¼ ë°˜í™˜.

    Args:
        data (pd.DataFrame): ë°ì´í„°í”„ë ˆì„.
        features (list): ë…ë¦½ ë³€ìˆ˜(íŠ¹ì§•) ë¦¬ìŠ¤íŠ¸.
        target_column (str): ì¢…ì† ë³€ìˆ˜ ì»¬ëŸ¼ ì´ë¦„.
        num_components (int): ì¶•ì†Œí•  ì°¨ì› ìˆ˜(ê¸°ë³¸ê°’: None, ìë™ ê²°ì •).
        scale_factor (float): ì£¼ì„±ë¶„ ì„ ë¶„ í¬ê¸° ì¡°ì • ë¹„ìœ¨(ê¸°ë³¸ê°’: 1).

    Returns:
        dict: PCA ê²°ê³¼ ë° ì‹œê°í™” ë°ì´í„°.
    """
    
    # ë°ì´í„° ì „ì²˜ë¦¬: ê²°ì¸¡ì¹˜ ì œê±°
    data.dropna(inplace=True)

    # 'ìˆ˜ì§‘ì‹œê°„'ì„ datetime í˜•íƒœë¡œ ë³€í™˜í•˜ê³  ì¸ë±ìŠ¤ë¡œ ì„¤ì •
    data['date_val'] = pd.to_datetime(data['date_val'])
    data.set_index('date_val', inplace=True)

    # ë…ë¦½ ë³€ìˆ˜ì™€ ì¢…ì† ë³€ìˆ˜ ë¶„ë¦¬
    X = data[features]  # ë…ë¦½ ë³€ìˆ˜
    y = data[target_column]  # ì¢…ì† ë³€ìˆ˜

    # ë°ì´í„° í‘œì¤€í™”
    scaler = StandardScaler()
    X_standardized = scaler.fit_transform(X)

    # í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„ë¦¬
    X_train, X_test, y_train, y_test = train_test_split(X_standardized, y, test_size=0.2, random_state=0)

    # PCA(ì£¼ì„±ë¶„ ë¶„ì„) ì§„í–‰
    pca = PCA()
    pca.fit(X_train)
    
    # ì„¤ëª… ê°€ëŠ¥í•œ ë¶„ì‚° ë¹„ìœ¨ ê³„ì‚°
    explained_variance_ratio = pca.explained_variance_ratio_
    cumulative_explained_variance = np.cumsum(explained_variance_ratio)

    # ì‚¬ìš©ì ì§€ì • ì°¨ì›ì´ ì—†ìœ¼ë©´(value == '') ì„¤ëª…ë¥ ì´ 90% ì´ìƒì¸ ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ
    if (num_components == '') or num_components is None:
        num_components = np.argmax(cumulative_explained_variance >= 0.9) + 1
    
    # num_components ê°’ í™•ì¸
    print(f"\nNumber of components explaining at least 90% of the variance: {num_components}")

    # ê²°ì •ëœ ì£¼ì„±ë¶„ ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ PCA ë‹¤ì‹œ ì ìš©
    pca_final = PCA(n_components=num_components)
    X_pca = pca_final.fit_transform(X_train)

    # 24.11.25 ì£¼ì„±ë¶„ ì„  íê¸°. PCAë¡œ ë³€í™˜ëœ ì°¨ì›ì˜ ì¶• ìì²´ê°€ ì´ë¯¸ PCì„ ì´ê¸° ë•Œë¬¸(ex. Xì¶• = PC1, Yì¶• = PC2, Zì¶• = PC3)
    # # ë°ì´í„° í‰ê· ê³¼ ì£¼ì„±ë¶„ ë²¡í„° ê°€ì ¸ì˜¤ê¸°
    # mean = np.mean(X_pca, axis=0)   # PCA ë³€í™˜ëœ ë°ì´í„°ì˜ ì¤‘ì‹¬(í‰ê· )
    # principal_vector = pca_final.components_  # ì¶•ì†Œëœ ì£¼ì„±ë¶„ ë²¡í„°

    # # ë™ì ìœ¼ë¡œ scale_factor ê³„ì‚°
    # if scale_factor is None:
    #     scale_factor = 10

    # # ì£¼ì„±ë¶„ ì¶• ê³„ì‚° (ì¶•ì†Œëœ ì°¨ì›ë§Œ ìˆœíšŒ, ìœ ì—°í•œ ì°¨ì› ì²˜ë¦¬)
    # principal_axes = []

    # # PCì„  ì¡°ì •
    # for i in range(num_components):
    #     scale_factor_adjusted = scale_factor * explained_variance_ratio[i]
    #     start = mean  # ë°ì´í„° ì¤‘ì‹¬
    #     end = mean + principal_vector[i, :num_components] * scale_factor_adjusted
    #     principal_axes.append(np.array([start, end]))

    return {
        'X_train': X_train,
        'X_test': X_test,
        'y_train': y_train,
        'y_test': y_test,
        'explained_variance_ratio': explained_variance_ratio,
        'cumulative_explained_variance': cumulative_explained_variance,
        'X_pca': X_pca,
        'num_components': num_components,
        # 'principal_axes': principal_axes
    }

# 24.07.23 ê¹€í•˜ëŠ˜ ì¶”ê°€ pca ì‹œê°í™” ë©”ì„œë“œ
def visualize_pca(pca, X, X_pca, X_new):
    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], alpha=0.2, label='Original Data')
    plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8, label='Reconstructed Data')

    # ì¶•ì†Œëœ ì°¨ì›ì´ 1ì°¨ì›ì¸ ê²½ìš° yê°’ì„ 0ìœ¼ë¡œ ì£¼ì–´ ì‹œê°í™”
    if X_pca.shape[1] == 1:
        plt.scatter(X_pca[:, 0], np.zeros_like(X_pca[:, 0]), alpha=0.5, label='PCA Reduced Data (1D)', c='red')

    plt.axis('equal')
    plt.legend()
    plt.title('PCA Visualization')
    
    # ì´ë¯¸ì§€ ì €ì¥ ë° ì „ì†¡
    plt.savefig('PCA.png')
    plt.show()
    
# 24.11.20 ê¹€í•˜ëŠ˜ ìˆ˜ì • pca ì™¸ ë‹¤ë¥¸ ê·¸ë˜í”„ë„ ì²˜ë¦¬ ê°€ëŠ¥í•˜ë„ë¡ ìˆ˜ì •
def visualize_to_plotly(title, graph1, graph2=None, graph3=None):
    # ì°¨ì›ì— ë”°ë¥¸ ê·¸ë˜í”„ ë°ì´í„° ì²˜ë¦¬
    def process_trace(graph):
        """1ì°¨ì›, 2ì°¨ì› ë˜ëŠ” 3ì°¨ì› ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ì—¬ Scatter/Scatter3D íŠ¸ë ˆì´ìŠ¤ ìƒì„±."""
        key = graph.get('key', None)

        if key == 'principal_axes':         # ì£¼ì„±ë¶„ ì¶• ë°ì´í„° ì²˜ë¦¬
            traces = []
            for i, axis in enumerate(graph['data']):   # ê° ì£¼ì„±ë¶„ ì¶• ë°˜ë³µ ì²˜ë¦¬
                dims = axis.shape[1]  # ë°ì´í„° ì°¨ì› í™•ì¸
                if dims == 1:  # 1D -> ì°¨ì› ì¶•ì†Œëœ ê°’
                    x = np.arange(1, len(axis[:, 0]) + 1)  # x ê°’ ìë™ ìƒì„±
                    y = axis[:, 0]
                    return go.Scatter(
                        x=x,
                        y=y,
                        mode='lines',
                        name=f'{graph["name"]} {i + 1}',
                        meta={'auto_x': False} 
                    )
                if dims == 2:  # 2D ì£¼ì„±ë¶„ ì¶•
                    x, y = axis[:, 0], axis[:, 1]
                    traces.append(go.Scatter(
                        x=x,
                        y=y,
                        mode='lines',
                        name=f'{graph["name"]} {i + 1}',
                        meta={'auto_x': False}  # xê°’ì´ ìë™ ìƒì„±ëœ ê²½ìš°ë¥¼ ë©”íƒ€ì •ë³´ë¡œ ì¶”ê°€
                    ))
                elif dims == 3:  # 3D ì£¼ì„±ë¶„ ì¶•
                    x, y, z = axis[:, 0], axis[:, 1], axis[:, 2]
                    traces.append(go.Scatter3d(
                        x=x,
                        y=y,
                        z=z,
                        mode='lines',
                        name=f'{graph["name"]} {i + 1}'
                    ))
                else:
                    raise ValueError(f"Unsupported dimensionality: {dims}D for principal_axes")

            return traces  # ì²˜ë¦¬ëœ ëª¨ë“  ì£¼ì„±ë¶„ ì¶• ë°˜í™˜       
        
        else:
            dims = graph['data'].ndim  # ë°ì´í„° ì°¨ì› í™•ì¸
            if dims == 1 : # 1ì°¨ì› ë°ì´í„°
                x = np.arange(1, len(graph['data']) + 1)  # x ê°’ ìë™ ìƒì„±
                y = graph['data']
                return go.Scatter(
                    x=x,
                    y=y,
                    mode=graph['mode'],
                    name=graph['name'],
                    meta={'auto_x': True}  # xê°’ì´ ìë™ ìƒì„±ëœ ê²½ìš°ë¥¼ ë©”íƒ€ì •ë³´ë¡œ ì¶”ê°€
                )
            if dims == 2 and graph['data'].shape[1] == 1:  # 1ì°¨ì› ë°ì´í„°(np array) -> ì°¨ì› ì¶•ì†Œëœ ê°’
                x = np.arange(1, len(graph['data'][:, 0]) + 1)  # x ê°’ ìë™ ìƒì„±
                y = graph['data'][:, 0]
                return go.Scatter(
                    x=x,
                    y=y,
                    mode=graph['mode'],
                    name=graph['name'],
                    meta={'auto_x': False} 
                )
            elif dims == 2 and graph['data'].shape[1] == 2:  # 2ì°¨ì› ë°ì´í„°
                x, y = graph['data'][:, 0], graph['data'][:, 1]
                return go.Scatter(
                    x=x,
                    y=y,
                    mode=graph['mode'],
                    name=graph['name'],
                    meta={'auto_x': False} 
                )
            elif dims == 2 and graph['data'].shape[1] == 3:  # 3ì°¨ì› ë°ì´í„°
                x, y, z = graph['data'][:, 0], graph['data'][:, 1], graph['data'][:, 2]
                return go.Scatter3d(
                    x=x,
                    y=y,
                    z=z,
                    mode='markers',
                    marker= dict(size=2),
                    # marker=dict(size= max(1, 10 - len(graph['data']) // 1000)),   # ë°ì´í„°ê°€ ë§ì„ìˆ˜ë¡ í¬ê¸° ê°ì†Œ
                    name=graph['name']
            )
            else:
                raise ValueError(f"Unsupported data dimensionality: {dims}D")


    # ë°ì´í„° ì²˜ë¦¬
    data = []
    for graph in [graph1, graph2, graph3]:
        if graph is not None:
            traces = process_trace(graph)
            if isinstance(traces, list):
                data.extend(traces)  # ë¦¬ìŠ¤íŠ¸ë©´ í’€ì–´ì„œ ì¶”ê°€
            else:
                data.append(traces)  # ë‹¨ì¼ ê°ì²´ë©´ ê·¸ëŒ€ë¡œ ì¶”ê°€


    # ë ˆì´ì•„ì›ƒ ì„¤ì •
    # xaxis ì„¤ì •
    xaxis = None
    if all(isinstance(trace, go.Scatter) for trace in data):  # 2D Scatterë§Œ í¬í•¨
        auto_x = all(trace.meta.get('auto_x', False) for trace in data)
        xaxis = dict(
            title=title.get('x', ''),
            tickmode='linear' if auto_x else 'auto',  # xê°’ì´ ìë™ ìƒì„±ëœ ê²½ìš°ì—ë§Œ linear ì„¤ì •
            nticks=20,  # ìµœëŒ€ ëˆˆê¸ˆ ê°œìˆ˜ ì œí•œ
        )

    layout = go.Layout(
        title={
            'text': title['main'], 
            'x': 0.5,               # chart title ê°€ìš´ë° ì •ë ¬
            'xanchor': 'center',    # ì¤‘ì•™ì— ì•µì»¤ ì„¤ì •
            'xref': 'paper',        # **ê·¸ë˜í”„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ì•™ ì •ë ¬
        },
        autosize = True,            # ìë™ í¬ê¸° ì¡°ì • í™œì„±í™”
        # autosize=False,           # ìë™ í¬ê¸° ì¡°ì • ë¹„í™œì„±í™”
        margin = dict(l=50, r=50, t=50, b=50),  # ì¢Œìš° ì—¬ë°± ìµœì†Œí™”ë¡œ ì œëª©ì„ ì •í™•íˆ ì¤‘ì•™ì— ìœ„ì¹˜
        xaxis = xaxis, 
        yaxis = dict(
            title = title.get('y', '')
        ) if all(isinstance(trace, go.Scatter) for trace in data) else None,
        scene = dict(  # 3D ë°ì´í„°ìš© ë ˆì´ì•„ì›ƒ
            xaxis_title=title.get('x', ''),
            yaxis_title=title.get('y', ''),
            zaxis_title=title.get('z', '')  # 3Dì¼ ê²½ìš° zì¶• ì œëª© ì¶”ê°€
        ) if any(isinstance(trace, go.Scatter3d) for trace in data) else None,  # 3D ì—¬ë¶€ í™•ì¸
        showlegend=True  # ê°•ì œë¡œ ë²”ë¡€ í‘œì‹œ
    )
    
    # figure ìƒì„±
    fig = go.Figure(data=data, layout=layout)
    
    # Plotly ê·¸ë˜í”„ë¥¼ HTMLë¡œ ë³€í™˜ (include_plotlyjs = JavaScript ë¼ì´ë¸ŒëŸ¬ë¦¬ í¬í•¨ ì—¬ë¶€ ì„¤ì •)
    # plotly_graph = pio.to_html(fig, full_html=False)  # ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì„ ë•Œ ì‚¬ìš©
    # cdn = ë¼ì´ë¸ŒëŸ¬ë¦¬ í¬í•¨í•´ì„œ ë³´ë‚´ê¸°. ì¶”í›„ì—ëŠ” falseë¡œ í•´ì„œ ë‚´ë¶€ì— ë¼ì´ë¸ŒëŸ¬ë¦¬ ì €ì¥í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•´ì•¼í•˜ì§€ ì•Šì„ê¹Œ
    plotly_graph = pio.to_html(fig, full_html=False, include_plotlyjs='cdn', config=dict(responsive=True))

    # Plotly ê·¸ë˜í”„ë¥¼ HTML íŒŒì¼ë¡œ ì €ì¥ (í•„ìš”ì‹œë§Œ í™œì„±í™”)
    # fig.write_html("D:\ê¹€í•˜ëŠ˜\ìœ„ì¡´\ì‹¤ë¬´\â˜…SFíŒ€\â˜…HLí´ë ˆë¬´ë¹„\ê°œë°œì‹¤ë¬´\ìŠ¤í„°ë””\plotly_to_html.html")
    # fig.write_html("D:\ê¹€í•˜ëŠ˜\ìœ„ì¡´\ì‹¤ë¬´\â˜…SFíŒ€\â˜…HLí´ë ˆë¬´ë¹„\ê°œë°œì‹¤ë¬´\ìŠ¤í„°ë””\plotly_to_html.html", include_plotlyjs='cdn')
    
    return plotly_graph    

def learning_data(context):
    items = []
    posparam = context.posparam
    gparam = context.gparam
    request = context.request
    action = gparam.get('action', 'read')
    user = context.request.user

    # data_svc = DataProcessingService()    # ì„ì‹œ ì£¼ì„(ë°ì´í„° ê°€ì ¸ì™€ì•¼ í•´ì„œ í•„ìš”í–ˆë˜ ê²ƒ)

    try:
        # PCAíƒ­ì—ì„œ ì¡°íšŒ ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ pca ì„¤ëª…ë¥  ë¶„í¬ë¥¼ ë³´ì—¬ì£¼ëŠ” ê·¸ë˜í”„ ì‹œê°í™” 
        # action ë¶„ë¦¬í•´ì„œ ì ìš©ë²„íŠ¼ ëˆŒë €ì„ ë•Œ ì‚¬ìš©ìì˜ ìš”êµ¬ì— ë§ëŠ” pcaë¥¼ ì§„í–‰
        if action == 'pca_evr':
            
            '''
            ì›ë˜ëŠ” ì‹œìŠ¤í…œì—ì„œ ë°ì´í„°ë¥¼ ì§ì ‘ ê°€ì ¸ì™€ì„œ ë¶„ì„ì„ ëŒë¦¬ëŠ” ë°©ì‹ì¸ ë“¯í•¨(data_processing.py)
             but,  1. í˜„ì¬ëŠ” í•´ë‹¹ ë°ì´í„°ì˜ ì›ë³¸ì¸ fileì´ ì—†ê³ ,
                   2. ì¿¼ë¦¬ì— í•´ë‹¹í•˜ëŠ” í…Œì´ë¸”ì— ì•„ì˜ˆ ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” row ìì²´ê°€ ì—†ëŠ” ìƒíƒœ.
             --> ì•„ë˜ ê³¼ì •ì—ì„œ DBë¡œ ë‚ ë¦¬ëŠ” ì¿¼ë¦¬ì˜ ê²°ê³¼ê°€ ì—†ì–´ì„œ(ë°ì´í„°ê°€ ì—†ì–´ì„œ) ì§„í–‰ì´ ë¶ˆê°€
             ë‹¤ë¥¸ í…Œì´ë¸”(ds_data)ì— ê°™ì€ ë°ì´í„°ë¡œ ì¶”ì •ë˜ëŠ” ë°ì´í„°ê°€ ë‚¨ì•„ìˆê¸´ í•¨

            # to_date = datetime.now()
            # from_date = datetime(2018, 12, 18) # ìˆ«ì í¬ë§·íŒ… 01, 02 ë°©ì‹ìœ¼ë¡œ ì£¼ë©´ ì—ëŸ¬ ë°œìƒí•¨
            # all_yn = "Y"

            # raw_data = data_svc.get_data(from_date, to_date, all_yn)
            # print("raw_data:/n", raw_data)            
            # data = pd.DataFrame.from_dict(raw_data)'''

            ##############################################################################
            
            # 24.08.08 ê¹€í•˜ëŠ˜ ì¶”ê°€ data ë¶ˆëŸ¬ì˜¤ê¸°(ë‹¤ë¥¸ ë©”ì„œë“œì—ì„œ ì¹´í”¼)
            md_id = gparam.get('md_id')
            daService = DaService('ds_data', md_id)
            num_components = gparam.get('num_components')
            data = daService.read_table_data()
            

            # ë¶„ì„ì— í•„ìš”í•œ ì»¬ëŸ¼ ì§€ì •í•˜ê¸°
            # ë…ë¦½ë³€ìˆ˜
            features = ['AP1', 'AP2', 'AP3', 'AP4', 'EC1', 'EC2', 'EC3', 'RAP1', 'RAP2', 'RAP3', 'RAP4']
            # ì¢…ì†ë³€ìˆ˜
            target_column = 'alarm'


            # PCA ì ìš© í•¨ìˆ˜ í˜¸ì¶œ
            pca_result = perform_pca(data, features, target_column, num_components)

            # plt ì‹œê°í™”ë¥¼ htmlë¡œ ë§Œë“¤ì–´ì£¼ëŠ” í•¨ìˆ˜ í˜¸ì¶œ
            '''title = {'main':'ê·¸ë˜í”„ ì œëª©', 'x':'xì¶• ì œëª©', 'y':'yì¶• ì œëª©'}'''
            title = {'main':'PCA ì£¼ì„±ë¶„ë³„ ì„¤ëª… ê°€ëŠ¥í•œ ë¶„ì‚° ë¹„ìœ¨', 'x':'ì£¼ì„±ë¶„(ì°¨ì›) ìˆ˜', 'y':'ì„¤ëª… ê°€ëŠ¥ ë¹„ìœ¨'}
            graph1 = {'data':pca_result['explained_variance_ratio'], 'mode':'lines+markers', 'name':'ì£¼ì„±ë¶„ë³„ ì„¤ëª… ë¹„ìœ¨'}
            graph2 = {'data':pca_result['cumulative_explained_variance'], 'mode':'lines+markers', 'name':'ëˆ„ì  ì„¤ëª… ë¹„ìœ¨'}

            pca_plotly = visualize_to_plotly(title, graph1, graph2)

            items = {'success':True, 'pca_plotly': pca_plotly}


        # ì‚¬ìš©ì ì§€ì • ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ(pca)
        elif action == 'pca_apply':        
            # ì‹œê°í™” 
            # ì‹œê°í™”ë¥¼ ìœ„í•´ PCA ì¶•ì†Œ ê²°ê³¼ë¥¼ ì›ë³¸ ë°ì´í„° ê³µê°„ìœ¼ë¡œ ì¬êµ¬ì„±
            # X_new = pca_final.inverse_transform(X_pca)
            
            # # ì‹œê°í™” í•¨ìˆ˜ í˜¸ì¶œí•˜ì—¬ pca ê²°ê³¼ í™•ì¸
            # visualize(pca_final, X, X_pca, X_new)
            
            md_id = gparam.get('md_id')
            daService = DaService('ds_data', md_id)
            num_components = gparam.get('num_components')
            data = daService.read_table_data()

            # PCA ì ìš© ë° ê²°ê³¼ ë°˜í™˜
            features = ['AP1', 'AP2', 'AP3', 'AP4', 'EC1', 'EC2', 'EC3', 'RAP1', 'RAP2', 'RAP3', 'RAP4']
            target_column = 'alarm'
            
            # ì‚¬ìš©ì ì§€ì • ì°¨ì› == null(ì„ íƒ ì•ˆí•¨)ì¼ ë•Œ ëˆ„ì  ì„¤ëª… ë¹„ìœ¨ì´ 90% ì´ìƒì¸ ì°¨ì›ìœ¼ë¡œ ìë™ ì ìš©
            # num_componentsë¥¼ intë¡œ ë³€í™˜í•˜ê¸°
            # print(f"num_components(type): {type(num_components)}")
            if num_components != None:
                num_components = int(num_components)

            pca_result = perform_pca(data, features, target_column, num_components)

            # plt ì‹œê°í™”ë¥¼ htmlë¡œ ë§Œë“¤ì–´ì£¼ëŠ” í•¨ìˆ˜ í˜¸ì¶œ
            '''title = {'main':'ê·¸ë˜í”„ ì œëª©', 'x':'xì¶• ì œëª©', 'y':'yì¶• ì œëª©'}'''
            title = {'main': 'PCAê°€ ì ìš©ëœ ê·¸ë˜í”„', 'x': 'PC1', 'y': 'PC2'}

            # ì‹œê°í™” ë°ì´í„° ì¤€ë¹„
            scatter_data = {
                'data': pca_result['X_pca'], 
                'name': f'{num_components}ì°¨ì› ë°ì´í„°', 
                'mode': 'markers'
                }
            # principal_axes_data = {     
            #     'key': 'principal_axes',  # ë©”íƒ€ì •ë³´
            #     'data': pca_result['principal_axes'],    # ì‹œê°í™” í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ name & mode ì¶”ê°€
            #     'name': 'ì£¼ì„±ë¶„ ì¶•',
            #     }

            pca_plotly = visualize_to_plotly(title, scatter_data)

            items = {'success':True, 'pca_result': pca_plotly}
            

        # ì¶”í›„ ìˆ˜ì •    
        elif action == 'create_model':
        
            # ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ ìƒì„±
            rf_model = RandomForestClassifier(n_estimators=100, random_state=0)

            # AdaBoost ì•™ìƒë¸” ëª¨ë¸ ìƒì„±
            model = AdaBoostClassifier(base_estimator=rf_model, n_estimators=50, random_state=0)
            pca_model = AdaBoostClassifier(base_estimator=rf_model, n_estimators=50, random_state=0)
            
            # ëª¨ë¸ í›ˆë ¨(ë¹„êµ)
            model.fit(X_train, y_train)     # ê¸°ë³¸ ëª¨ë¸     
            pca_model.fit(X_pca, y_train)   # PCA ì ìš© ëª¨ë¸

            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡
            predictions = model.predict(X_test) # ê¸°ë³¸ ëª¨ë¸ ì˜ˆì¸¡
            pca_X_test = pca_final.transform(X_test)    # ì˜ˆì¸¡ì„ ìœ„í•´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë„ PCA ì§„í–‰
            pca_predictions = pca_model.predict(pca_X_test) # PCA ëª¨ë¸ ì˜ˆì¸¡
            
            # ì˜ˆì¸¡ ê²°ê³¼ì˜ ì •í™•ë„ í‰ê°€
            accuracy = accuracy_score(y_test, predictions)
            print(f'ê¸°ë³¸ëª¨ë¸ test Accuracy: {accuracy:.2f}')
            pca_accuracy = accuracy_score(y_test, pca_predictions)
            print(f'PCAëª¨ë¸ test Accuracy: {pca_accuracy:.2f}')


            # ì‹¤ì œ ë°ì´í„° ì˜ˆì¸¡
            # ë¯¸ë˜ 5ë¶„ í›„ì˜ ì•ŒëŒê°’ ì˜ˆì¸¡ì„ ìœ„í•´ ë§ˆì§€ë§‰ 30ê°œì˜ ê´€ì¸¡ì¹˜ ì¤€ë¹„
            future_data = X[-30:]

            # ë¯¸ë˜ ê°’ ì˜ˆì¸¡
            future_predictions = model.predict(future_data)         # ê¸°ë³¸ëª¨ë¸
            pca_future_data = pca_final.transform(future_data)      # ê´€ì¸¡ë°ì´í„° PCA ì ìš©
            pca_future_predictions = pca_model.predict(pca_future_data) # PCA ëª¨ë¸ ì˜ˆì¸¡

            # # ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
            # for i, prediction in enumerate(future_predictions, 1):
            #     print(f'ê¸°ë³¸ëª¨ë¸: 5ë¶„ í›„ ì•ŒëŒê°’ ì˜ˆì¸¡ {i*10}ì´ˆ: {prediction}')
            
            # ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
            for i, (basic_pred, pca_pred, real_alarm) in enumerate(zip(future_predictions, pca_future_predictions, y[-30:]), 1):
                print(f'5ë¶„ í›„ ì•ŒëŒê°’ ì˜ˆì¸¡ {i*10}ì´ˆ:\n --> ê¸°ë³¸ëª¨ë¸: {basic_pred}   /   PCAì ìš©ëª¨ë¸: {pca_pred}   /   ì‹¤ì œ alarm ê°’: {real_alarm}')

            # í›ˆë ¨ ë° ì˜ˆì¸¡ì´ ì™„ë£Œë˜ë©´ ê²°ê³¼ê°’ ì„œë²„ì— ì „ë‹¬
            items = {'success':True}
            
            
        ''' ë°ì´í„°íŒŒì¼ì €ì¥ : save_ds_model
            ë°ì´í„°íŒŒì¼ëª©ë¡ì¡°íšŒ : ds_data_list
            rowë°ì´í„°ì¡°íšŒ : ds_rows_list
            ë°ì´í„°íŒŒì¼ë‚´ìš©ì¡°íšŒ : ds_model_info. prop_data ì—†ìœ¼ë©´ csvíŒŒì¼ ì½ê¸°.
            íŒŒì¼ì—ì„œ ë°ì´í„° ì½ì–´ì„œ DBì €ì¥   make_db_from_file
            DBì—ì„œ ì½ì–´ì„œ ì»¬ëŸ¼ì •ë³´ ë§Œë“¤ê³  ì €ì¥ make_col_info
            cate_col_list
            ë°ì´í„°ì»¬ëŸ¼ëª©ë¡(ë¶„í¬)ì¡°íšŒ : ds_col_list
            ë°ì´í„°ì»¬ëŸ¼ì €ì¥(ì „ì²˜ë¦¬): save_ds_col_preprocess
            ë°ì´í„°ë¶„í¬ê·¸ë˜í”„ì¡°íšŒ(ìˆ˜ì¹˜í˜•) : ds_numcol_boxhist
            ë²”ì£¼í˜•ë°ì´í„°íˆìŠ¤í† ê·¸ë¨(ë²”ì£¼í˜•) : ds_col_count_plot
            ìƒê´€ê´€ê³„íˆíŠ¸ë§µ(ìˆ˜ì¹˜í˜•) : ds_heatmap
            ë°ì´í„°ì»¬ëŸ¼XYì§€ì • : save_ds_col_xy
            ë°ì´í„°ì‚°ì ë„ì¡°íšŒ : ds_col_scatter
            ìƒê´€ê´€ê³„ê°’ì¡°íšŒ : ds_var_corr_sheet
            íšŒê·€ë¶„ì„ì‹ ì¡°íšŒ : ds_y_regression_list
            /* 25.02.10 ê¹€í•˜ëŠ˜ ì¶”ê°€ */
            ëª¨ë¸ë§ˆìŠ¤í„°ì¡°íšŒ: ds_model_tree_list
        '''
        # 25.02.10 ê¹€í•˜ëŠ˜ ì¶”ê°€
        if action == "ds_model_tree_list":
            master_type = gparam.get('master_type')
            keyword = gparam.get('keyword')

            sql = '''
            WITH A AS (
                SELECT md.id, md."Name" AS name, md."Description" AS description, md."Type" AS type, md._created
                FROM ds_model md
            ),
            B AS (
                SELECT A.id,
                       COUNT(mc.*) AS var_count
                FROM A
                INNER JOIN ds_model_col mc ON mc."DsModel_id" = A.id
                GROUP BY A.id
            ),
            F AS (
                SELECT A.id, af.id AS file_id, af."FileName" AS file_name
                FROM A
                INNER JOIN attach_file af ON af."DataPk" = A.id 
                AND af."TableName" = 'ds_model'
            ),
            Parent AS (
                -- ë¶€ëª¨ ë°ì´í„° (í•­ìƒ í‘œì‹œ)
                SELECT
                    CONCAT('M_', mm.id) AS tree_id,
                    NULL AS tree_master_id,
                    mm.id AS master_id,    
                    CAST(NULL AS INTEGER) AS model_id,  -- ğŸ”¥ ìë£Œí˜• ë§ì¶¤
                    mm."Name" AS name,
                    mm."Type" AS type,
                    NULL AS source,
                    NULL AS ver,
                    NULL AS description,
                    NULL AS file_path,
                    NULL AS algorithm_type,
                    mm."_created" AS _created,
                    CAST(NULL AS INTEGER) AS var_count,  -- ğŸ”¥ ìë£Œí˜• ë§ì¶¤
                    NULL AS file_name,
                    CAST(NULL AS INTEGER) AS file_id     -- ğŸ”¥ ìë£Œí˜• ë§ì¶¤
                FROM ds_master mm
                WHERE 1=1
                '''
            if master_type:
                sql += '''
                AND UPPER(mm."Type") = UPPER(%(master_type)s)
                '''

            sql += '''
            ),
            Child AS (
                -- ìì‹ ë°ì´í„° (ê²€ìƒ‰ì–´ ìˆì„ ë•Œë§Œ í•„í„°ë§)
                SELECT
                    CONCAT('C_', md.id) AS tree_id,
                    CONCAT('M_', md."DsMaster_id") AS tree_master_id,
                    md."DsMaster_id" AS master_id,
                    md.id AS model_id,
                    md."Name" AS name,
                    md."Type" AS type,
                    -- mm."Type" AS master_type,
                    md."SourceName" AS source,
                    md."Version" AS ver,
                    md."Description" AS description,
                    md."FilePath" AS file_path,
                    tc."AlgorithmType" AS algorithm_type,
                    md."_created" AS _created,
                    B.var_count AS var_count,  
                    F.file_name AS file_name,  
                    F.file_id AS file_id
                FROM ds_model md
                LEFT JOIN ds_tag_corr tc ON tc."DsModel_id" = md.id
                LEFT JOIN ds_master mm ON mm.id = md."DsMaster_id"
                LEFT JOIN B ON B.id = md.id  
                LEFT JOIN F ON F.id = md.id  
                WHERE 1=1
            '''

            if keyword:
                sql += '''
                AND (
                    UPPER(md."Type") LIKE CONCAT('%%', UPPER(%(keyword)s),'%%')
                    OR UPPER(md."Name") LIKE CONCAT('%%', UPPER(%(keyword)s),'%%')
                    OR UPPER(md."Description") LIKE CONCAT('%%', UPPER(%(keyword)s),'%%')
                )
                '''

            sql += '''
            )
            SELECT * FROM Parent
            WHERE 1=1
            '''
            if keyword:
                sql += '''
                AND master_id IN (SELECT master_id FROM Child)
                '''
            sql += '''
            UNION ALL
            SELECT * FROM Child
            '''

            # if keyword:
            #     sql += '''
            #     AND (tree_master_id IN (SELECT tree_id FROM Parent) OR tree_id IN (SELECT tree_id FROM Child))
            #     '''

            sql += '''
            ORDER BY tree_master_id NULLS FIRST, name ASC;
            '''

            # sql = '''
            # WITH A AS (
            #     SELECT md.id, md."Name" AS name, md."Description" AS description, md."Type" AS type, md._created
            #     FROM ds_model md
            # ),
            # B AS (
            #     SELECT A.id,
            #            COUNT(mc.*) AS var_count
            #     FROM A
            #     INNER JOIN ds_model_col mc ON mc."DsModel_id" = A.id
            #     GROUP BY A.id
            # ),
            # F AS (
            #     SELECT A.id, af.id AS file_id, af."FileName" AS file_name
            #     FROM A
            #     INNER JOIN attach_file af ON af."DataPk" = A.id 
            #     AND af."TableName" = 'ds_model'
            # ),
            # T AS (
            #     SELECT
            #         CONCAT('M_', mm.id) AS tree_id,
            #         NULL AS tree_master_id,
            #         mm.id AS master_id,    
            #         NULL AS model_id,
            #         mm."Name" AS name,
            #         mm."Type" AS type,
            #         mm."Type" AS master_type,
            #         NULL AS source,
            #         NULL AS ver,
            #         NULL AS description,
            #         NULL AS file_path,
            #         NULL AS algorithm_type,
            #         mm."_created" AS _created,
            #         NULL AS var_count,  -- ds_masterì—ëŠ” var_count ì—†ìŒ
            #         NULL AS file_name,  -- ds_masterì—ëŠ” file_name ì—†ìŒ
            #         NULL AS file_id     -- ds_masterì—ëŠ” file_id ì—†ìŒ
            #     FROM ds_master mm
            #     UNION ALL
            #     SELECT
            #         CONCAT('C_', md.id) AS tree_id,
            #         CONCAT('M_', md."DsMaster_id") AS tree_master_id,
            #         md."DsMaster_id" AS master_id,
            #         md.id AS model_id,
            #         md."Name" AS name,
            #         md."Type" AS type,
            #         mm."Type" AS master_type,
            #         md."SourceName" AS source,
            #         md."Version" AS ver,
            #         md."Description" AS description,
            #         md."FilePath" AS file_path,
            #         tc."AlgorithmType" AS algorithm_type,
            #         md."_created" AS _created,
            #         B.var_count AS var_count,  -- ë³€ìˆ˜ ê°œìˆ˜ ì¶”ê°€
            #         F.file_name AS file_name,  -- íŒŒì¼ëª… ì¶”ê°€
            #         F.file_id AS file_id
            #     FROM ds_model md
            #     LEFT JOIN ds_tag_corr tc ON tc."DsModel_id" = md.id
            #     LEFT JOIN ds_master mm ON mm.id = md."DsMaster_id"
            #     LEFT JOIN B ON B.id = md.id  -- ë³€ìˆ˜ ê°œìˆ˜ ì¡°ì¸ (ds_model_col ë°˜ì˜)
            #     LEFT JOIN F ON F.id = md.id  -- íŒŒì¼ëª… ì¡°ì¸
            #     WHERE 1=1
            # '''
            # if keyword:
            #     sql += '''
            #     AND (
            #         UPPER(md."Type") LIKE CONCAT('%%', UPPER(%(keyword)s),'%%')
            #         OR UPPER(md."Name") LIKE CONCAT('%%', UPPER(%(keyword)s),'%%')
            #         OR UPPER(md."Description") LIKE CONCAT('%%', UPPER(%(keyword)s),'%%')
            #     )
            #     '''
            # if master_type:
            #     sql += '''
            #     AND UPPER(master_type) = UPPER(%(master_type)s)
            #     '''
            # sql += '''
            # ORDER BY tree_master_id NULLS FIRST, name ASC;
            # '''

            # sql = '''
            # SELECT
            #     mm.id AS master_id
            #     , mm."Name" AS master_name
            #     , mm."Type" AS master_type
            #     , mm."_created"
            #     , COALESCE(md.id, mm.id) AS id
            #     , md."Name" AS name
            #     , md."Type" AS type
            #     , md."SourceName" AS source
            #     , md."Version" AS ver
            #     , md."Description" AS description
            #     , md."FilePath" AS file_path
            #     , tc."AlgorithmType" AS algorithm_type
            #     , md."_created" AS model_created
            # FROM
            #     ds_master mm                
            # LEFT OUTER JOIN
            #     ds_model md ON md."DsMaster_id" = mm.id
            # LEFT OUTER JOIN
            #     ds_tag_corr tc ON tc."DsModel_id" = md.id
            # WHERE 1=1
            # '''
            # if master_type:
            #     sql += '''
            #     AND UPPER(mm."Type") = UPPER(%(master_type)s)
            #     '''
            # if keyword:
            #     sql +='''
            #     AND (
            #         UPPER(mm."Name") LIKE CONCAT('%%', UPPER(%(keyword)s),'%%')
            #         OR UPPER(mm."Type") LIKE CONCAT('%%', UPPER(%(keyword)s),'%%')
            #     )
            #     '''
            # sql += '''ORDER BY mm."Type", mm."Name", md."Name" desc'''

            dc = {}
            dc['master_type'] = master_type
            dc['keyword'] = keyword
        
            items = DbUtil.get_rows(sql, dc)  

        if action == 'save_ds_model':
            # md_id = posparam.get('id')
            mm_id = CommonUtil.try_int(posparam.get('mm_id'))
            md_id = CommonUtil.try_int(posparam.get('id'))
            Name = posparam.get('Name')
            Description = posparam.get('Description')
            Type = posparam.get('Type')
            new_file_id = posparam.get('fileId')
            
            old_file_id = -1

            if md_id:
                md = DsModel.objects.get(id=md_id)
            else:
                md = DsModel()
            md.Name = Name
            md.Description = Description
            md.Type = Type
            md.DsMaster_id = mm_id
            md.set_audit(user)
            md.save()
            md_id = md.id

            # set table_name, data_pk
            daService = DaService('ds_model', md_id)

            # attach_fileì˜ dataPkë¥¼ ì—…ë°ì´íŠ¸
            if new_file_id:
                fileService = AttachFileService()
                fileService.updateDataPk(new_file_id, md_id)
 
            items = {'success': True}

        elif action == 'ds_data_list':
            keyword = gparam.get('keyword')           
            date_from = gparam.get('date_from')
            date_to = gparam.get('date_to')

            sql=''' 
            with A as (
	            select 
                    dd.id
                    , dd."Name" as name
                    , dd."Description" as description
                    , dd."Type" as type
                    , dd._created
	            from ds_model dd
	            where 1 = 1
            ), B as (
	            select 
                    A.id
	                , count(dc.*) as var_count
	                , count(case when dc."X" = 1 then 1 end) as x_count
	                , count(case when dc."Y" = 1 then 1 end) as y_count
	            from A 
	            inner join ds_model_col dc on dc."DsModel_id" = A.id
	            group by a.id
            ), F as (
	            select 
                    A.id
                    , af.id as file_id
                    , af."FileName" as file_name
                    , af."PhysicFileName" 
	            from A 
	            inner join attach_file af on af."DataPk" = A.id 
	            and af."TableName" = 'ds_data'
            )
            select 
                A.*
                , F.file_id
                , F.file_name
                , F."PhysicFileName" 
                , B.var_count
                , B.x_count
                , B.y_count
            from A 
            left join B on B.id = A.id 
            left join F on F.id = A.id
            order by A._created
            '''
            dc = {}
            items = DbUtil.get_rows(sql, dc)


        elif action == 'ds_rows_list':
            '''
            '''
            md_id = CommonUtil.try_int(gparam.get('md_id'))

            sql = ''' 
            SELECT 
                mc.id
                , mc."VarIndex"
                , mc."VarName"
            FROM ds_model_col mc
            WHERE mc."DsModel_id" = %(md_id)s
            ORDER BY mc."VarIndex"  
            '''
            dc = {}
            dc['md_id'] = md_id

            xrows = DbUtil.get_rows(sql, dc)

            sql = ''' SELECT dt."RowIndex" '''
            for i, x in enumerate(xrows):
                var_name = x['VarName']
                sql += ''' 
                , MIN(CASE 
                    WHEN dt."Code" = \'''' + var_name + '''\' 
                    THEN dt."Char1" 
                    END) AS col_''' + str(i+1)
            sql += ''' FROM ds_model_data dt
            WHERE dt."DsModel_id" = %(md_id)s
            GROUP BY dt."RowIndex"
            ORDER BY dt."RowIndex"
            '''
            dc = {}
            dc['md_id'] = md_id

            rows = DbUtil.get_rows(sql, dc)
            items = {
                'success': True,
                'xrows': xrows,
                'rows': rows
            }

        elif action == 'ds_model_info':
            ''' 
            '''
            md_id = gparam.get('md_id')
            sql = ''' 
            select 
                md.id
                , md."Name"
                , md."Description"
                , md."Type"
                , md._created
                , (select id as file_id 
                    from attach_file af 
		            where af."DataPk" = md.id 
		            and af."TableName" = 'ds_model' 
                    order by id desc limit 1) as file_id
            from ds_model md
            where md.id = %(md_id)s
            '''
            dc = {}
            dc['md_id'] = md_id
            row = DbUtil.get_row(sql, dc)

            return row
           

        elif action == 'make_db_from_file':
            ''' prop_data ì—†ìœ¼ë©´ csvíŒŒì¼ ì½ì–´ì„œ prop_dataì— ì €ì¥í•˜ê³  ë³€ìˆ˜ì»¬ëŸ¼ ì €ì¥.
            '''
            md_id = CommonUtil.try_int(gparam.get('md_id'))
            # md_id = gparam.get('md_id')
            sql = ''' 
            SELECT 
                md.id
                , md."Name"
                , md."Description"
                , md."Type"
                , md._created
                , (SELECT id as file_id 
                    FROM attach_file af
                    WHERE af."DataPk" = md.id 
		            AND af."TableName" = 'ds_model' order by id desc limit 1) as file_id
            FROM ds_model md
            WHERE md.id = %(md_id)s
            '''
            dc = {}
            dc['md_id'] = md_id
            row = DbUtil.get_row(sql, dc)

            file_id = row['file_id']
            if not file_id:
                return {'success':False, 'message':'íŒŒì¼ì—†ìŒ' }

            daService = DaService('ds_model', md_id)

            #q = DsDataTable.objects.filter(DsModel_id=md_id)
            #pd = q.first()
            #if pd:
            #    df = daService.read_table_data()
            #else:
            #    df = daService.read_csv2()

            #df = daService.read_table_data()
            #if not df:
            df = daService.read_csv2()

            daService.make_col_info(df)

            return {'success':True, 'message':'' }


        elif action == 'cate_col_list':
            ''' ì»¬ëŸ¼ì •ë³´ë¥¼ ì½ëŠ”ë‹¤.
            '''
            md_id = gparam.get('md_id')
            sql = ''' select dc.id, dc."VarName" as value, dc."VarName" as text
            from ds_col dc
            where dc."DsModel_id" = %(md_id)s
            and dc."CategoryCount" > 0
            order by dc."VarIndex"  
            '''
            dc = {}
            dc['md_id'] = md_id

            items = DbUtil.get_rows(sql, dc)

            return items

        # ìš°ì„  ì‚¬ìš© ë³´ë¥˜. 
        elif action == 'make_col_info':
            ''' table ë°ì´í„° ì½ì–´ì„œ ì»¬ëŸ¼ì •ë³´ë¥¼ ë§Œë“¤ì–´ì„œ ì €ì¥í•œë‹¤.
            '''
            md_id = CommonUtil.try_int(gparam.get('md_id'))
            #md_id = gparam.get('md_id')

            daService = DaService('ds_model', md_id)
            df = daService.read_table_data()
            daService.make_col_info(df)

            return {'success':True, 'message':'' }

        elif action == 'ds_col_list':
            '''
            '''
            md_id = CommonUtil.try_int(gparam.get('md_id'))
            sql = ''' 
            SELECT 
                mc.id
                , mc."VarIndex"
                , mc."VarName"
                , mc."DataCount"
                , mc."MissingCount"
                , mc."CategoryCount" 
                , mc."Mean" 
                , mc."Std"
                , mc."Q1"
                , mc."Q2"
                , mc."Q3" 
                , mc."MissingValProcess"
                , mc."DropOutLow"
                , mc."DropOutUpper"
                , mc."X"
                , mc."Y" 
            FROM ds_model_col mc
            WHERE mc."DsModel_id" = %(md_id)s
            ORDER BY mc."VarIndex"  
            '''
            dc = {}
            dc['md_id'] = md_id

            items = DbUtil.get_rows(sql, dc)
         
        elif action == 'save_ds_col_preprocess':
            md_id = posparam.get('md_id')
            Q = posparam.get('Q')
            #Q = json.loads(Q)
            
            daService = DaService('ds_model', md_id)
            df = daService.read_table_data()
            
            # ì»¬ëŸ¼ë³„ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë°©ì‹ & ì´ìƒì¹˜ ê¸°ì¤€ì„ ì €ì¥í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ìƒì„±
            column_preprocess_info = {}

            for item in Q:
                VarName = item['VarName'] 
                MissingValProcess = CommonUtil.blank_to_none(item['MissingValProcess'])
                DropOutLow = CommonUtil.try_float(item['DropOutLow'])
                DropOutUpper = CommonUtil.try_float(item['DropOutUpper'])

                column_preprocess_info[VarName] = {
                    "MissingValProcess": MissingValProcess,
                    "DropOutLow": DropOutLow,
                    "DropOutUpper": DropOutUpper
                }

                q = DsModelColumn.objects.filter(DsModel_id=md_id, VarIndex=item['VarIndex'])
                dc = q.first()
                if dc:
                    dc.MissingValProcess = MissingValProcess
                    dc.DropOutLow = DropOutLow
                    dc.DropOutUpper = DropOutUpper
                    dc.save()

                # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
                if MissingValProcess == 'drop':
                    df[VarName].dropna(inplace=True)
                elif MissingValProcess == 'mean':
                    df[VarName].fillna(df[VarName].mean(), inplace=True)
                elif MissingValProcess == 'median':
                    df[VarName].fillna(df[VarName].median(), inplace=True)
                elif MissingValProcess == 'mode':
                    # mode()ì˜ returnì´ ë°°ì—´ì¼ ìˆ˜ ìˆìŒ(ìµœë¹ˆê°’ì´ ì—¬ëŸ¬ê°œ)
                    mode_value = df[VarName].mode()
                    df[VarName].fillna(mode_value[0] if not mode_value.empty else None, inplace=True)

                #ì´ìƒì¹˜ ì œê±°
                if DropOutLow:
                    df = df[df[VarName] >= DropOutLow]  # â¬… `VarName`ì„ ì»¬ëŸ¼ì²˜ëŸ¼ ì‚¬ìš©í•´ì•¼ í•¨
                if DropOutUpper:
                    df = df[df[VarName] <= DropOutUpper]
 
            # ds_model_data delete, update
            sql = ''' 
            DELETE 
	        FROM ds_model_data
	        WHERE "DsModel_id"  = %(md_id)s
	            AND "RowIndex" IN (
		            SELECT DISTINCT "RowIndex"
		            FROM ds_model_data dt
		            WHERE "DsModel_id" = %(md_id)s
		            AND "Code" IN (
			            SELECT "VarName" 
			            FROM ds_model_col mc
			            WHERE "DsModel_id" = %(md_id)s
			                AND "MissingValProcess" = 'drop'
		            )
		            -- AND "Char1" IS NULL  -- Char1 ê°’ì´ nanì´ë‚˜ NaNì¸ ê²½ìš° ì—…ë°ì´íŠ¸ ì•ˆë¨
                    AND (dt."Char1" IS NULL OR UPPER(dt."Char1") IN ('NAN', ''))
	            )
            '''
            dc = {}
            dc['md_id'] = md_id
            ret = DbUtil.execute(sql, dc)

            sql = '''
            WITH A AS (
	            SELECT 
                    mc."DsModel_id" AS md_id
                    , "VarName"
                    , (CASE "MissingValProcess" 
                        WHEN 'mean' THEN "Mean" 
                        WHEN 'median' THEN "Q2" 
                        END) new_val
	            FROM ds_model_col mc
	            WHERE "DsModel_id" = %(md_id)s
	                AND "MissingValProcess" IN ('mean', 'median')
            )
            UPDATE ds_model_data 
            SET 
                "Char1" = CASE 
                            WHEN (UPPER(ds_model_data."Char1") IN ('NAN', '') OR ds_model_data."Char1" IS NULL)
                            THEN CAST(A.new_val AS TEXT)
                            ELSE ds_model_data."Char1"
                          END,
                "Number1" = CASE 
                            WHEN (ds_model_data."Number1" IS NULL OR ds_model_data."Number1" = 'NaN'::float) 
                            THEN A.new_val
                            ELSE ds_model_data."Number1"
                          END
            FROM A
            WHERE ds_model_data."DsModel_id" = A.md_id 
                AND ds_model_data."Code" = A."VarName"
                -- AND ds_model_data."Char1" IS NULL   -- Char1 ê°’ì´ nanì´ë‚˜ NaNì¸ ê²½ìš° ì—…ë°ì´íŠ¸ ì•ˆë¨
                AND A.new_val IS NOT NULL 
            '''

            dc = {}
            dc['md_id'] = md_id
            ret = DbUtil.execute(sql, dc)

            daService.make_col_info(df, column_preprocess_info)

            items = {'success': True, 'message':''}

        elif action == 'ds_numcol_boxhist':
            ''' ìˆ˜ì¹˜í˜• ë°ì´í„°ì— ëŒ€í•´ì„œ íˆìŠ¤í† ê·¸ë¨, ìƒììˆ˜ì—¼ê·¸ë¦¼ ê·¸ë¦¬ê¸°
            '''
            import matplotlib.pyplot as plt
            #from matplotlib.figure import Figure

            md_id = gparam.get('md_id')
            daService = DaService('ds_data', md_id)
            df = daService.read_table_data()

            num_df = df.select_dtypes(include=['int64','float64'])

            #nrow = math.ceil( len(num_df.columns) / (ncol) )
            nrow = len(num_df.columns)
            
            fig, ax = plt.subplots(nrows=nrow, ncols=2, figsize=(20, 20))
            #fig, ax = plt.subplots(nrows=nrow, ncols=2)
            #fig.set_figheight(100)
            #fig.set_figwidth(80)
            #fig.tight_layout()
            #fig = Figure()
            #ax = fig.subplots(nrow, ncol)

            for index, key in enumerate(num_df.columns):
                col = num_df[key]
                r = index
                c = 0
                ax1 = ax[r][c]
                ax[r][c].boxplot(col)
                ax[r][c].set_title(key)
                ax[r][c].hlines(y=col.quantile(0.001),color='blue',xmin=0,xmax=2) 
                ax[r][c].hlines(y=col.quantile(0.999),color='red',xmin=0,xmax=2) 

                ax2 = ax[r][c+1]
                ax2.hist(col,bins=10)
                #ax2.vlines(x=col.quantile(0.001),color='blue',ymin=0,ymax=1000) 
                #ax2.vlines(x=col.quantile(0.999),color='red',ymin=0,ymax=1000)
            
            chart_url = daService.plt_url(fig)
            #plt.show()

            return {'success':True, 'chart_url': chart_url}

            import matplotlib.pyplot as plt
            import seaborn as sns

            plt.hist(df_sat["ì–´ë¦°ì´"], bins=10)  # bins: ëª‡ê°œì˜ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆŒ ê²ƒì¸ê°€ -> ë§‰ëŒ€ì˜ ìˆ˜
            plt.show()

            plt.boxplot(df_2018_apr["ì–´ë¦°ì´"])
            plt.show()

            sns.histplot(data=df_sat, x="ì–´ë¥¸")
            plt.show()

            fig, ax = plt.subplots()
            # ë¬¸ì œ 13.

            # ax.histë¥¼ ì´ìš©í•´ì„œ ê°€ê²©(price)ì˜ ë¶„í¬ë¥¼ êµ¬í•©ë‹ˆë‹¤. ì´ë•Œ, bins=10, color='indigo'ì…ë‹ˆë‹¤.
            ax.hist(df['price'], bins=10, color='indigo')
            ax.set_xlabel('price')
            ax.set_ylabel('count')
            ax.set_facecolor('white')

            sns.boxplot(data=df_2018_apr, y="ì–´ë¦°ì´")

            ncol = 4
            nrow = 10
            fig, ax = plt.subplots(nrows=nrow, ncols=ncol, figsize=(20, 40))

            for i, X_Feature in enumerate(X_Features):
                row = i // ncol
                col = (i % ncol)
                ax1 = ax[row, col]
                ax1.hist(train_X[X_Feature],bins=100)    
                ax1.set_title(X_Feature)
            plt.show()
           

        elif action == 'ds_col_count_plot':
            ''' ë²”ì£¼í˜• íˆìŠ¤í† ê·¸ë¨
            '''
            import matplotlib.pyplot as plt
            import seaborn as sns
            #from matplotlib.figure import Figure

            md_id = gparam.get('md_id')
            variables = gparam.get('variables')
            daService = DaService('ds_data', md_id)
            df = daService.read_table_data()
            
            cate=[]
            var_list = variables.split(',')
            if len(var_list) > 0:
                for key in var_list:
                    if df[key].dtype=='O':
                        cate.append(key)
            else:
                for key in df.columns:
                    if df[key].dtype=='O':
                        cate.append(key)

            #fig = plt.figure(figsize=(15,10))
            cate_count = len(cate)
            #fig, ax = plt.subplots(cate_count, 1, constrained_layout=True)
            fig, ax = plt.subplots(cate_count, 1, tight_layout=True)
            plt.subplots_adjust(hspace=0.5)

            for index, key in enumerate(cate): #ë²”ì£¼í˜• ë°ì´í„°ì— ëŒ€í•œ ë³€ìˆ˜ëª… ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
                # Hint. sns.countplotì„ ì´ìš©í•©ë‹ˆë‹¤. hue ì˜µì…˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
                #sns.countplot(x=i, data=df, hue=hue_column)
                if cate_count == 1:
                    axis = ax
                else:
                    axis = ax[index]
                sns.countplot(x=key, data=df, ax=axis)
                #ax[index].set_title(key)
                #plt.xticks(rotation=45)
            #plt.subplots_adjust(hspace=10)

            chart_url = daService.plt_url(fig)
            #plt.show()

            return {'success':True, 'chart_url': chart_url}


        elif action == 'ds_heatmap':
            ''' ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
            '''
            import matplotlib.pyplot as plt
            import seaborn as sns
            #from matplotlib.figure import Figure

            md_id = gparam.get('md_id')
            daService = DaService('ds_data', md_id)
            df = daService.read_table_data()

            #num_df = df.select_dtypes(include=['int64','float64'])

            corr_df = df.corr()
            # seabornì„ ì‚¬ìš©í•˜ì—¬ heatmap ì¶œë ¥

            fig = plt.figure(figsize=(15,10))
            sns.heatmap(corr_df, annot=True, cmap='PuBu')
            #fig = Figure()
           
            chart_url = daService.plt_url(fig)
            #plt.show()

            return {'success':True, 'chart_url': chart_url}


        elif action == 'save_ds_col_xy':
            '''
            '''
            md_id = posparam.get('md_id')
            Q = posparam.get('Q')
            #Q = json.loads(Q)

            daService = DaService('ds_model', md_id)
            df = daService.read_table_data()

            x_cols = []
            y_cols = []
            for item in Q:
                q = DsModelColumn.objects.filter(DsModel_id=md_id)
                q = q.filter(VarIndex=item['VarIndex'])
                dc = q.first()
                dc.X = 1 if item['X'] else None
                dc.Y = 1 if item['Y'] else None
                dc.save()

                if item['X']:
                   x_cols.append(item['VarName'])
                if item['Y']:
                   y_cols.append(item['VarName'])

            items = { 'success': True }

            simplelinear = LinearRegression()
            multilinear = LinearRegression()

            #corr = df[x_cols + y_cols].corr()
            q = DsTagCorrelation.objects.filter(DsModel_id=md_id)
            q.delete()

            for y in y_cols:
                multilinear.fit(df[x_cols], df[y])
                beta_0 = multilinear.intercept_
                beta_i_list = multilinear.coef_

                cr = DsTagCorrelation()
                cr.DsModel_id = md_id
                cr.YVarName = y
                cr.XVarName = 'intercept_'
                cr.MultiLinearCoef = beta_0
                cr.save()

                for i, x in enumerate(x_cols):
                    try:
                        corr =df[x].corr(df[y])
                    except Exception as e:
                        corr = None
                    cr = DsTagCorrelation()
                    cr.DsModel_id = md_id
                    cr.XVarName = x
                    cr.YVarName = y
                    cr.r = corr
                    cr.MultiLinearCoef = beta_i_list[i]
                    
                    #if corr and (corr > 0.5 or 1==1):  #0.5
                    if True:  #0.5
                        try:
                            simplelinear.fit(df[[x]], df[y])
                            beta_0 = simplelinear.intercept_
                            beta_1 = simplelinear.coef_ 

                            equ = 'y = ' + str(beta_1) + '*x +' + str(beta_0)
                            cr.RegressionEquation = equ
                        except Exception as e:
                            pass
                    cr.save()

        elif action == 'ds_col_scatter':
            '''
            '''
            import seaborn as sns

            md_id = gparam.get('md_id')
            daService = DaService('ds_model', md_id)
            df = daService.read_table_data()

            #num_df = df.select_dtypes(include=['int64','float64'])
            x_vars, y_vars = daService.xy_columns()
            #x_vars = ['Age','Fare']
            #y_vars = ['Fare']
            
            #fig = Figure() # snsì—ì„œëŠ” ì´ êµ¬ë¬¸ì„ ì‹¤í–‰í•˜ì§€ ì•Šì•„ì•¼ ìƒˆ ì°½ì´ ì•ˆ ëœ¸.
            #ax = fig.subplots()

            if len(x_vars) > 0 and len(y_vars) > 0:
                pp = sns.pairplot(df, x_vars=x_vars, y_vars=y_vars, diag_kind='hist')
            else:
                pp = sns.pairplot(df, diag_kind='hist') # ê·¸ë˜í”„ê°€ ë³„ë„ ì°½ì— í‘œì‹œëœë‹¤.

            scatter_url = daService.plt_url(pp)
            return {'success':True, 'scatter_url': scatter_url} 

            import matplotlib.pyplot as plt

            #sns.scatterplot(data=df,x="ì „ìš©ë©´ì (í‰)",y="ê±°ë˜ê¸ˆì•¡(ë§Œì›)")
            #plt.savefig("problem_4.png") # ì±„ì ì„ ìœ„í•œ ì½”ë“œì…ë‹ˆë‹¤.

            #df_corr=df.corr(numeric_only=True)
            #sns.heatmap(df_corr, annot=True)

            pp = sns.pairplot(stage1_data, x_vars=stage1_data.columns[[14,19,24,29,34]], y_vars=stage1_data.columns[[39,44,49]], diag_kind='hist')

            #pair plotì„ ê·¸ë ¤ì¤ë‹ˆë‹¤.
            for ax in pp.axes.flatten():
                ax.xaxis.label.set_rotation(90) #xlabelê³¼ ylabelë¥¼ íšŒì „ì‹œí‚µë‹ˆë‹¤.
                ax.yaxis.label.set_rotation(0)
                ax.yaxis.label.set_ha('right')

            #xlabelê³¼ ylabelì˜ í¬ê¸°ë¥¼ í‚¤ì›ë‹ˆë‹¤.
            [plt.setp(item.xaxis.get_label(), 'size', 20) for item in pp.axes.ravel()] 
            [plt.setp(item.yaxis.get_label(), 'size', 20) for item in pp.axes.ravel()]

            plt.show()

        elif action == 'ds_var_corr_sheet':
            ''' ìƒê´€ê³„ìˆ˜ ì¡°íšŒ
                ê°€ë¡œì¶• : x1, x2
                ì„¸ë¡œì¶• : yë³€ìˆ˜
                cell 1: r ê°’.
                cell 2: ë‹¤ì¤‘íšŒê·€ì‹

            '''
            md_id = gparam.get('md_id')

            sql = ''' select dc.id, dc."VarIndex", dc."VarName"
            from ds_col dc
            where dc."DsModel_id" = %(md_id)s
            and dc."X" = 1
            order by dc."VarIndex"  
            '''
            dc = {}
            dc['md_id'] = md_id

            xrows = DbUtil.get_rows(sql, dc)

            sql = ''' select vc."YVarName", 1 as grp_idx, 'ìƒê´€ê³„ìˆ˜' as data_type, null::numeric as intercept_ '''
            for i, x in enumerate(xrows):
                var_name = x['VarName']
                sql += ''' 
                ,min(case when vc."XVarName" = \'''' + var_name + '''\' then vc.r end)::text as x''' + str(i+1)
            sql += ''' from ds_var_corr vc 
            where vc."DsModel_id" = %(md_id)s
            group by vc."YVarName" 
            union all
            select vc."YVarName", 2 as grp_idx, 'ë‹¨ì¼íšŒê·€ì‹' as data_type
            , null::numeric as intercept_ '''
            for i, x in enumerate(xrows):
                var_name = x['VarName']
                sql += ''' 
                ,min(case when vc."XVarName" = \'''' + var_name + '''\' then vc."RegressionEquation" end) as x''' + str(i+1)
            sql += ''' from ds_var_corr vc 
            where vc."DsModel_id" = %(md_id)s
            group by vc."YVarName" 
            union all
            select vc."YVarName", 3 as grp_idx, 'ë‹¤ì¤‘íšŒê·€ì‹ê³„ìˆ˜' as data_type
            , min(case when vc."XVarName" = 'intercept_' then vc."MultiLinearCoef" end) as intercept_ '''
            for i, x in enumerate(xrows):
                var_name = x['VarName']
                sql += ''' 
                ,min(case when vc."XVarName" = \'''' + var_name + '''\' then vc."MultiLinearCoef" end)::text as x''' + str(i+1)
            sql += ''' from ds_var_corr vc 
            where vc."DsModel_id" = %(md_id)s
            group by vc."YVarName" 
            order by 1, 2
            '''
            dc = {}
            dc['md_id'] = md_id
            rows = DbUtil.get_rows(sql, dc)
            
            items = {
                'success': True,
                'xrows': xrows,
                'rows': rows
            }

        elif action == 'ds_y_regression_list':

            md_id = gparam.get('md_id')

            sql = ''' select dc.id, dc."VarIndex", dc."VarName"
            from ds_col dc
            where dc."DsModel_id" = %(md_id)s
            and dc."X" = 1
            order by dc."VarIndex"  
            '''
            dc = {}
            dc['md_id'] = md_id

            xrows = DbUtil.get_rows(sql, dc)

            sql = ''' select vc."YVarName" '''
            for i, x in enumerate(xrows):
                var_name = x['VarName']
                sql += ''' 
                ,min(case when vc."XVarName" = \'''' + var_name + '''\' then vc."RegressionEquation" end) as re''' + str(i)
            sql += ''' from ds_var_corr vc 
            where vc."DsModel_id" = %(md_id)s
            group by vc."YVarName" 
            '''
            dc = {}
            dc['md_id'] = md_id
            rows = DbUtil.get_rows(sql, dc)    
            
            items = rows
        
    except Exception as ex:
        source = '/api/ai/learning_data, action:{}'.format(action)
        LogWriter.add_dblog('error', source , ex)
        # # 24.07.16 ê¹€í•˜ëŠ˜ ì—ëŸ¬ ë¡œê·¸ í™•ì¸
        # print('error: ', ex)
        items = {'success':False}

    return items
